{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roll number: 2023121013, 2023122002, 2023121006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Instructions\n",
    " * Fill in the roll-number in the cell above.\n",
    " * Code must be submitted in Python in jupyter notebooks. We highly recommend using anaconda/miniconda distribution or at the minimum, virtual environments for this assignment.\n",
    " * All the code and result files should be uploaded in the github classroom.\n",
    " *  Most of the questions require you to **code your own functions** unless there is a need to call in the abilities of the mentioned libraries, such as Visualisation from Open3D. Make sure your code is modular since you will be reusing them for future assignments. All the functions related to transformation matrices, quaternions, and 3D projection are expected to be coded by you.\n",
    " *  All the representations are expected to be in a right-hand coordinate system.\n",
    "<!--  * Answer to the descriptive questions should be answered in your own words. Copy-paste answers will lead to penalty. -->\n",
    " * You could split the Jupyter Notebook cells where TODO is written, but please try to avoid splitting/changing the structure of other cells.\n",
    " * All the visualization should be done inside the notebook unless specified otherwise.\n",
    " * Plagiarism will lead to heavy penalty.\n",
    " * Commit the notebooks in the repo and any other results files under the result folder in the GitHub Classroom repo. \n",
    " * This is a group assignment. Discussions are encouraged but any sharing of code among different teams will be penalized. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 1: ICP with SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Perform Procrustes alignment on two point clouds with (given) known correspondences. (5 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let X be your point cloud observed from the initial pose P1. You then transform it to a new pose P2. Now you wish to apply ICP to recover transformation between (X & P1) and (X & P2).\n",
    "\n",
    "Use toothless.ply point cloud and perform the alignment between the two point clouds using procrustes alignment. Your task is to write a function that takes two point clouds as input wherein the corresponding points between the two point clouds are located at the same index and returns the transformation matrix between them. Compute the alignment error after aligning the two point clouds.\n",
    "\n",
    "<b>Use root mean squared error (RSME) as the alignment error metric.</b>\n",
    "\n",
    "Make sure your code is modular as we will use this function in the next sub-part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will again use our own getTransform function to generate the 4x4 Transformation matrix to transform the pointcloud to P2, so make sure your code works for any general Transformation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import open3d as o3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTransform():\n",
    "    \"\"\" Generates a random 4x4 transformation matrix. \"\"\"\n",
    "\n",
    "    R = o3d.geometry.get_rotation_matrix_from_xyz(np.random.rand(3) * 2 * np.pi)\n",
    "    T = np.random.rand(3) * 5000\n",
    "    transformation = np.vstack((np.hstack((R, T.reshape(-1, 1))), [0, 0, 0, 1]))\n",
    "    return transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_transformation(points, transformation):\n",
    "    \"\"\" Applies a 4x4 transformation matrix to an array of points. \"\"\"\n",
    "\n",
    "    rotation = transformation[:3, :3]\n",
    "    translation = transformation[:3, 3]\n",
    "    transformed_points = (points @ rotation.T) + translation\n",
    "    return transformed_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Transformation:\n",
      " [[-5.76600617e-01  7.42965817e-01  3.39902227e-01  3.21665218e+03]\n",
      " [ 7.60254369e-01  3.35528968e-01  5.56267566e-01  1.20546238e+03]\n",
      " [ 2.99240743e-01  5.79156375e-01 -7.58309219e-01  4.33474833e+03]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n",
      "Predicted Transformation:\n",
      " [[-5.76600617e-01  7.42965817e-01  3.39902227e-01  3.21665218e+03]\n",
      " [ 7.60254369e-01  3.35528968e-01  5.56267566e-01  1.20546238e+03]\n",
      " [ 2.99240743e-01  5.79156375e-01 -7.58309219e-01  4.33474833e+03]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n",
      "Alignment Error: 2.832500664323631e-11\n"
     ]
    }
   ],
   "source": [
    "def icp_known_correspondences(T):\n",
    "    \"\"\" Given a transformation matrix, transforms toothless.ply\n",
    "    and recovers the transformation using Procrustes alignment.\"\"\"\n",
    "\n",
    "    # Read the point cloud\n",
    "    pcd = o3d.io.read_point_cloud(\"data/toothless.ply\")\n",
    "    X_P1 = np.asarray(pcd.points)\n",
    "\n",
    "    # Apply the transformation\n",
    "    X_P2 = apply_transformation(X_P1, T)\n",
    "\n",
    "    # Predict the applied transformation\n",
    "    T_pred = procrustes_alignment(X_P1, X_P2)\n",
    "\n",
    "    # Apply the predicted transformation\n",
    "    X_P2_pred = apply_transformation(X_P1, T_pred)\n",
    "\n",
    "    # Compute RMSE between the two transformed point clouds\n",
    "    error = np.sqrt(np.mean((X_P2 - X_P2_pred) ** 2))\n",
    "\n",
    "    return T_pred, error\n",
    "\n",
    "def procrustes_alignment(P, Q):\n",
    "    \"\"\" Performs Procrustes alignment on two point clouds\n",
    "    and returns the transformation matrix.\"\"\"\n",
    "\n",
    "    # Compute the mean of the point clouds\n",
    "    P_mean = np.mean(P, axis=0)\n",
    "    Q_mean = np.mean(Q, axis=0)\n",
    "\n",
    "    # Center the two point clouds around their means\n",
    "    P_centered = P - P_mean\n",
    "    Q_centered = Q - Q_mean\n",
    "\n",
    "    # Compute the cross-covariance matrix\n",
    "    W = Q_centered.T @ P_centered\n",
    "\n",
    "    # Compute the SVD of the cross-covariance matrix\n",
    "    U, S, V_t = np.linalg.svd(W)\n",
    "\n",
    "    # Compute the rotation and translation\n",
    "    R = U @ V_t\n",
    "    T = Q_mean - (R @ P_mean)\n",
    "\n",
    "    # Construct the transformation matrix\n",
    "    transformation = np.vstack((np.hstack((R, T.reshape(-1, 1))), [0, 0, 0, 1]))\n",
    "\n",
    "    return transformation\n",
    "\n",
    "T = getTransform()\n",
    "T_pred, error = icp_known_correspondences(T)\n",
    "\n",
    "print('Original Transformation:\\n', T)\n",
    "print('Predicted Transformation:\\n', T_pred)\n",
    "print('Alignment Error:', error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Implement ICP algorithm with unknown correspondences. (5 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to write a function that implements ICP and takes two point clouds as input wherein the correspondances are unknown. Visualize the pointclouds and plot their individual coordinate frames as you perform ICP over them. Compute the alignment error in each iteration. \n",
    "\n",
    "Refer to Shubodh's notes to compute correspondences: https://saishubodh.notion.site/Mobile-Robotics-Navigating-from-Theory-to-Application-0b65a9c20edd4081978f4ffad917febb?p=a25686ce1a11409d838d47bcac43ab4b&pm=s#bb9aaf2e316b4db3b399df1742f0444c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error (iteration 0): 39.8568\n",
      "Error (iteration 1): 30.7565\n",
      "Error (iteration 2): 22.7827\n",
      "Error (iteration 3): 19.0444\n",
      "Error (iteration 4): 18.6785\n",
      "Error (iteration 5): 17.9875\n",
      "Error (iteration 6): 15.4331\n",
      "Error (iteration 7): 16.3381\n",
      "Error (iteration 8): 17.8410\n",
      "Error (iteration 9): 15.4440\n",
      "Error (iteration 10): 16.0850\n",
      "Error (iteration 11): 14.4375\n",
      "Error (iteration 12): 13.6350\n",
      "Error (iteration 13): 14.6319\n",
      "Error (iteration 14): 13.8031\n",
      "Error (iteration 15): 15.0569\n",
      "Error (iteration 16): 13.6038\n",
      "Error (iteration 17): 15.2525\n",
      "Error (iteration 18): 14.0732\n",
      "Error (iteration 19): 13.6789\n",
      "Error (iteration 20): 13.6911\n",
      "Error (iteration 21): 13.3054\n",
      "Error (iteration 22): 13.0538\n",
      "Error (iteration 23): 14.9965\n",
      "Error (iteration 24): 14.0090\n",
      "Error (iteration 25): 13.4618\n",
      "Error (iteration 26): 13.0616\n",
      "Error (iteration 27): 13.6686\n",
      "Error (iteration 28): 13.7932\n",
      "Error (iteration 29): 13.4190\n",
      "Error (iteration 30): 13.2500\n",
      "Error (iteration 31): 13.1303\n",
      "Error (iteration 32): 12.7872\n",
      "Error (iteration 33): 14.2652\n",
      "Error (iteration 34): 14.7539\n",
      "Error (iteration 35): 13.8659\n",
      "Error (iteration 36): 12.5972\n",
      "Error (iteration 37): 13.0073\n",
      "Error (iteration 38): 12.4597\n",
      "Error (iteration 39): 15.4919\n",
      "Error (iteration 40): 13.1576\n",
      "Error (iteration 41): 13.4086\n",
      "Error (iteration 42): 14.2180\n",
      "Error (iteration 43): 13.6311\n",
      "Error (iteration 44): 13.2955\n",
      "Error (iteration 45): 13.3889\n",
      "Error (iteration 46): 12.4853\n",
      "Error (iteration 47): 12.2179\n",
      "Error (iteration 48): 13.4987\n",
      "Error (iteration 49): 11.9784\n",
      "Error (iteration 50): 13.8898\n",
      "Error (iteration 51): 12.2130\n",
      "Error (iteration 52): 14.0190\n",
      "Error (iteration 53): 15.3511\n",
      "Error (iteration 54): 12.9943\n",
      "Error (iteration 55): 12.3942\n",
      "Error (iteration 56): 13.5514\n",
      "Error (iteration 57): 13.0773\n",
      "Error (iteration 58): 14.0640\n",
      "Error (iteration 59): 12.7851\n",
      "Error (iteration 60): 13.1577\n",
      "Error (iteration 61): 13.8404\n",
      "Error (iteration 62): 13.4280\n",
      "Error (iteration 63): 14.1722\n",
      "Error (iteration 64): 14.1859\n",
      "Error (iteration 65): 14.3449\n",
      "Error (iteration 66): 12.7317\n",
      "Error (iteration 67): 13.2654\n",
      "Error (iteration 68): 13.1899\n",
      "Error (iteration 69): 14.2062\n",
      "Error (iteration 70): 11.8947\n",
      "Error (iteration 71): 12.4438\n",
      "Error (iteration 72): 12.6905\n",
      "Error (iteration 73): 15.9932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imDefLkup.c,419: The application disposed a key event with 243 serial.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error (iteration 74): 13.9956\n",
      "Error (iteration 75): 13.2354\n",
      "Error (iteration 76): 13.6306\n",
      "Error (iteration 77): 14.8691\n",
      "Error (iteration 78): 13.5894\n",
      "Error (iteration 79): 13.2454\n",
      "Error (iteration 80): 13.0204\n",
      "Error (iteration 81): 13.4585\n",
      "Error (iteration 82): 13.4737\n",
      "Error (iteration 83): 13.5682\n",
      "Error (iteration 84): 13.9847\n",
      "Error (iteration 85): 14.1159\n",
      "Error (iteration 86): 14.2394\n",
      "Error (iteration 87): 13.7116\n",
      "Error (iteration 88): 12.6581\n",
      "Error (iteration 89): 13.0712\n",
      "Error (iteration 90): 13.1473\n",
      "Error (iteration 91): 13.9199\n",
      "Error (iteration 92): 13.7482\n",
      "Error (iteration 93): 12.5802\n",
      "Error (iteration 94): 13.2218\n",
      "Error (iteration 95): 14.8380\n",
      "Error (iteration 96): 13.2062\n",
      "Error (iteration 97): 12.5720\n",
      "Error (iteration 98): 14.7747\n",
      "Error (iteration 99): 15.4270\n",
      "Original Transformation:\n",
      " [[ 1.01688227e-01 -4.38081414e-01 -8.93165259e-01  4.93242590e+03]\n",
      " [ 1.37746845e-01 -8.82970497e-01  4.48763756e-01  2.74485769e+03]\n",
      " [-9.85233633e-01 -1.68664687e-01 -2.94433721e-02  2.23820105e+03]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n",
      "Predicted Transformation:\n",
      " [[ 4.94895773e-01 -3.12833439e-01 -8.10687001e-01  6.31970086e+03]\n",
      " [ 2.51110574e-01  9.44630565e-01 -2.11226362e-01 -9.32991310e+01]\n",
      " [ 8.31878388e-01 -9.90370442e-02  5.46049458e-01 -9.01892862e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n",
      "Alignment Error: 15.426972005159886\n"
     ]
    }
   ],
   "source": [
    "def icp_unknown_correspondences(P, Q, num_iterations=100):\n",
    "    \"\"\" Given two transformation matrices, transforms and shuffles toothless.ply\n",
    "    and performs alignment on the transformed point clouds. \"\"\"\n",
    "\n",
    "    # Read the point cloud\n",
    "    pcd = o3d.io.read_point_cloud(\"data/toothless.ply\")\n",
    "    X = np.asarray(pcd.points)\n",
    "\n",
    "    # Apply the transformation\n",
    "    X_P = apply_transformation(X, P)\n",
    "    X_Q = apply_transformation(X, Q)\n",
    "\n",
    "    # Shuffle the second point cloud for unknown correspondences\n",
    "    np.random.shuffle(X_Q)\n",
    "\n",
    "    # Visualize the point clouds\n",
    "    vis = o3d.visualization.Visualizer()\n",
    "    vis.create_window()\n",
    "    PC_X_P = o3d.geometry.PointCloud()\n",
    "    PC_X_Q = o3d.geometry.PointCloud()\n",
    "    PC_X_P.points = o3d.utility.Vector3dVector(X_P)\n",
    "    PC_X_Q.points = o3d.utility.Vector3dVector(X_Q)\n",
    "    vis.add_geometry(PC_X_P)\n",
    "    vis.add_geometry(PC_X_Q)\n",
    "    vis.poll_events()\n",
    "    vis.update_renderer()\n",
    "\n",
    "    # Initialize the error and predicted transformation\n",
    "    error = 0\n",
    "    T_pred = np.eye(4)\n",
    "\n",
    "    for iteration in range(num_iterations):\n",
    "        # Subsample point clouds\n",
    "        X_P_sampled = X_P[np.random.choice(X_P.shape[0], 1000, replace=False)]\n",
    "        X_Q_sampled = X_Q[np.random.choice(X_Q.shape[0], 5000, replace=False)]\n",
    "\n",
    "        # Determine corresponding pairs and align the sampled point cloud\n",
    "        distance, indices = get_nearest_neighbors(X_P_sampled, X_Q_sampled)\n",
    "        X_Q_sampled = X_Q_sampled[indices]\n",
    "\n",
    "        # Predict the applied transformation\n",
    "        T_pred_current = procrustes_alignment(X_P_sampled, X_Q_sampled)\n",
    "        T_pred = T_pred_current @ T_pred\n",
    "\n",
    "        # Apply the predicted transformation\n",
    "        X_P = apply_transformation(X_P, T_pred_current)\n",
    "        X_P_sampled = apply_transformation(X_P_sampled, T_pred_current)\n",
    "\n",
    "        # Update the visualization\n",
    "        PC_X_P.points = o3d.utility.Vector3dVector(X_P)\n",
    "        vis.update_geometry(PC_X_P)\n",
    "        vis.poll_events()\n",
    "        vis.update_renderer()\n",
    "\n",
    "        # Compute RMSE between the two subsampled point clouds\n",
    "        error = np.sqrt(np.mean((X_P_sampled - X_Q_sampled) ** 2))\n",
    "        print(f'Error (iteration {iteration}): {error:.4f}')\n",
    "\n",
    "    # Terminate visualization\n",
    "    vis.run()\n",
    "    vis.destroy_window()\n",
    "\n",
    "    return T_pred, error\n",
    "\n",
    "def get_nearest_neighbors(P, Q):\n",
    "    dist = np.linalg.norm(P[:, np.newaxis] - Q, axis=2)\n",
    "    return np.min(dist, axis=1), np.argmin(dist, axis=1)\n",
    "\n",
    "P = getTransform()\n",
    "Q = getTransform()\n",
    "T_pred, error = icp_unknown_correspondences(P, Q)\n",
    "\n",
    "print('Original Transformation:\\n', Q)\n",
    "print('Predicted Transformation:\\n', T_pred)\n",
    "print('Alignment Error:', error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 2: ICP with Lie Groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Predict the Transformation matrix between 2 point clouds with known correspondences (15 Points)\n",
    "Perform the same task as 1.1 using Lie Group Optimization from scratch to predict the transformation between the 2 point clouds.\n",
    "\n",
    "Refer: https://saishubodh.notion.site/Mobile-Robotics-Navigating-from-Theory-to-Application-0b65a9c20edd4081978f4ffad917febb?p=ee55fe5689794693910ab7861bef067b&pm=s#7b82d84766a84b63b91d859579e4886b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = getTransform()\n",
    "def icp_with_lie(T):\n",
    "    #### YOUR CODE HERE ####\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 3: Pose Graph Optimization with G2O\n",
    "### Objective (5 Points)\n",
    "A robot is travelling in a oval trajectory. It is equipped with wheel odometry for odometry information and RGBD sensors for loop closure information. Due to noise in wheel odometry it generates a noisy estimate of the trajectory. Our task is to use loop closure pairs to correct the drift.\n",
    "\n",
    "We pose this problem as a pose graph optimization problem. In our graph, poses are the vertices and constraints are the edges. \n",
    "\n",
    "References:\n",
    "\n",
    "1.) Class notes: https://saishubodh.notion.site/G2O-Edge-Types-d9f9ff63c77c4ceeb84b1e49085004e3\n",
    "\n",
    "2.) Cyrill Stachniss lecture: https://www.youtube.com/watch?v=uHbRKvD8TWg "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Given: \n",
    "In practical scenarios, we'd obtain the following from our sensors after some post-processing:\n",
    "\n",
    "1. Initial position\n",
    "2. Odometry Contraints/Edges: This \"edge\" information tells us relative transformation between two nodes. These two nodes are consecutive in the case of Odometry but not in the case of Loop Closure (next point).\n",
    "3. Loop Closure Contraints/Edges: Remember that while optimizing, you have another kind of \"anchor\" edge as you've seen in 1. solved example.\n",
    "\n",
    "You have been given a text file named `edges.txt` (in `data/`) which has all the above 3 and it follows G2O's format (as explained in class, [link here](https://saishubodh.notion.site/G2O-Edge-Types-d9f9ff63c77c4ceeb84b1e49085004e3) ). The ground truth is `gt.txt`.\n",
    "\n",
    "Install g2o as mentioned in `g2o.ipynb` and optimise `edges.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evo (10 Points)\n",
    "We need a measure of how good the trajectory is. The error/loss used earlier doesn't tell us much about how the trajectory differs from the ground truth. Here, we try to do just this - compute error metrics. Rather than computing these from scratch, we will just Evo - https://github.com/MichaelGrupp/evo/.\n",
    "\n",
    "Look at the absolute pose error (APE) and relative pose error (RPE). What do they capture and how are they calculated (descriptive answer)? How do these metrics differ in methodology? Can we determine if the error is more along the x/y axis?\n",
    "\n",
    "Answer the above questions and report errors for the obtained trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mapfree",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
