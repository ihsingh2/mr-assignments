{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roll number: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Instructions\n",
    " * Fill in the roll-number in the cell above.\n",
    " * Code must be submitted in Python in jupyter notebooks. We highly recommend using anaconda/miniconda distribution or at the minimum, virtual environments for this assignment.\n",
    " * All the code and result files should be uploaded in the github classroom.\n",
    " * For this assignment, you will be using Open3D  extensively. Refer to [Open3D](http://www.open3d.org/docs/release/) documentation.\n",
    " *  Most of the questions require you to **code your own functions** unless there is a need to call in the abilities of the mentioned libraries, such as Visualisation from Open3D. Make sure your code is modular since you will be reusing them for future assignments. All the functions related to transformation matrices, quaternions, and 3D projection are expected to be coded by you.\n",
    " *  All the representations are expected to be in a right-hand coordinate system.\n",
    "<!--  * Answer to the descriptive questions should be answered in your own words. Copy-paste answers will lead to penalty. -->\n",
    " * You could split the Jupyter Notebook cells where TODO is written, but please try to avoid splitting/changing the structure of other cells.\n",
    " * All the visualization should be done inside the notebook unless specified otherwise.\n",
    " * Plagiarism will lead to heavy penalty.\n",
    " * Commit the notebooks in the repo and any other results files under the result folder in the GitHub Classroom repo. \n",
    " * Commits past the deadline will not be considered.\n",
    " * This is a group assignment. Discussions are encouraged but any sharing of code among different teams will be penalized. \n",
    "\n",
    "### Instructions for group formation\n",
    " * We have circulated google sheet in moodle to fill in team members. Please finalize the teams formation by 18th Aug (tentative deadline). Same teams will be working towards project and other 2 Assignments as well. \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1: Transformations and Projections on Autonomous Driving Dataset (20 Points)\n",
    "\n",
    "In this question, you will work with real world autonomous driving dataset (sequence in Waymo dataset). The dataset has LiDAR point clouds, images. You are required to demonstrate: \n",
    "\n",
    "**I. Various transformations of rotation matrices as described in below tasks.**\n",
    "\n",
    "**II. Visualization as a result of above transformations in Open3D**\n",
    "\n",
    "## Given data:\n",
    "\n",
    "1.) `LiDAR Point Clouds` : Stored at each timestep in the folder `lidar`. The point clouds are provided in the ego frame attached to lidar sensor (vehicle's reference frame).\n",
    "\n",
    "2.) `Images` : Stored at each timestep in the folder `images`. \n",
    "\n",
    "**Naming Convention** : {timestep}_{cam_no}.jpg where timestep is specified in 3 digits and cam_no : [0, 1, 2] indicates centre, left and right camera respectively.\n",
    "\n",
    "3.) `Camera-to-Ego Transformations`: Stored in the folder `cam2ego`, which converts points from each camera's reference frame to the vehicle's (or ego) reference frame.\n",
    "\n",
    "4.) `Ego-to-World Transformations`: Stored in the folder `ego2world`, which converts points from the vehicle's reference frame to the world frame W.\n",
    "\n",
    "5.) `Camera Intrinsics`: Stored in the folder `intrinsics` provided for 3 cameras.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions to read lidar data and camera instrinsics are provided below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to read instrinsic matrix\n",
    "\n",
    "import numpy as np\n",
    "import os \n",
    "\n",
    "def read_intrinsic(timestep):\n",
    "    intrinsic = np.loadtxt(f\"sample_intrinsic_{timestep}.txt\")\n",
    "    fx, fy, cx, cy = intrinsic[0], intrinsic[1], intrinsic[2], intrinsic[3]\n",
    "    intrinsic_matrix = np.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]])\n",
    "\n",
    "read_intrinsic(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(165454, 3)\n"
     ]
    }
   ],
   "source": [
    "# Helper function to read lidar data at timestep 0 (same logic to read lidars at all remaining timesteps)\n",
    "\n",
    "lidar_data = np.memmap('sample_lidar_data_000.bin',\n",
    "                dtype=np.float32,\n",
    "                mode=\"r\",\n",
    "            ).reshape(-1, 14)   # (165454, 14)\n",
    "\n",
    "lidar_origins = lidar_data[:, :3]\n",
    "lidar_points = lidar_data[:, 3:6]   # (165454, 3)\n",
    "lidar_ids = lidar_data[:, -1]   # (165454,)\n",
    "\n",
    "# Lidar points to be used \n",
    "print(lidar_points.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Note:** Even though Waymo dataset has 5 cameras, you are given the dataset corresponding to middle 3 cameras only. Please ignore other 2 cameras.\n",
    "\n",
    "## Notation for tasks:\n",
    "\n",
    "a.) `Global Reference Frame G`: Defined as the first ego frame (i.e., the translation vector of ego2world[0] is the origin of frame G in world frame W).\n",
    "\n",
    "World Frame W: A fixed world reference frame.\n",
    "\n",
    "b.) `Ego Frame`: Attached to the LiDAR and changes as the vehicle moves.\n",
    "\n",
    "c.) `Camera Frames`: Each of the 5 cameras has its own frame, which changes as the vehicle moves.\n",
    "\n",
    "Note: Axis directions of `Ego Frame` and `Camera Frames` are aligned with the Waymo Coordinate System (LiDAR) described below\n",
    "\n",
    "## Coordinate Systems:\n",
    "\n",
    "**OpenCV Coordinate System:** x right, y down, z front.\n",
    "\n",
    "**Waymo Coordinate System (LiDAR):** x front, y left, z up.\n",
    "\n",
    "\n",
    "![Waymo Setup](./waymo_setup.jpg \"Waymo Setup\")\n",
    "\n",
    "Link to dataset (one sequence) : https://drive.google.com/drive/folders/17YDx2Yn1KmPjmlaHsoFz4Jpa8zpgovO2?usp=drive_link\n",
    "\n",
    "If you want to try on other sequences as well, please refer to : https://waymo.com/open/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Task 1`. Transformations of LiDAR Point Clouds (10 points)\n",
    "\n",
    "**Instructions:** \n",
    "\n",
    "Transform the LiDAR point clouds at all timesteps to the global reference frame G. Concatenate these transformed point clouds.\n",
    "    \n",
    "Visualization: Use Open3D to visualize the concatenated point cloud in the global reference frame G. Also, display the concatenation process at every timestep starting from first point cloud\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# TODO: TASK 1\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Task 2`. Projecting LiDAR Point Clouds onto images (10 points)\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "Transform the concatenated point cloud from task 1 to the frame of each of the 3 cameras at timesteps `0, 20, and 55`. \n",
    "    \n",
    "Project these transformed point clouds onto the respective camera frames using the provided camera intrinsics. Concatenated point cloud would be very dense, so randomly select arbitrary number of points for better visualization. \n",
    "\n",
    "**Projected image pixel x : K * X_3d where X_3d is the 3d point in camera frame.**\n",
    "    \n",
    "Visualization: Overlay the projected points onto the camera images and visualize them.\n",
    "\n",
    "**For example:** Overlayed concatenated point cloud on camera `000_0.png` and `030_2.png` are shown below\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"./projected_000_0.png\" alt=\"Drawing\" style=\"width: 750px;\"/> </td>\n",
    "<td> <img src=\"./projected_030_2.png\" alt=\"Drawing\" style=\"width: 750px;\"/> </td>\n",
    "</tr></table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# TODO: TASK 2\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "\n",
    "### `Task 3`. Compute Depth Image from Projected Point Cloud in camera frame (5 points)\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "Using the projected point clouds to camera frame from task 2, visualize the depth image by considering only the z-coordinate of the projected points in the camera frame.\n",
    "\n",
    "Visualization: Display the depth image for each of the 3 cameras at timesteps `0, 20, and 55` alongside the corresponding RGB image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# TODO: TASK 3\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: You might be asked to show the above results for different timesteps and from one of the 3 cameras during evaluation/viva."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2: Various Representations for Rotations and Gimbal lock (15 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Euler angles (2.5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Write a function that returns a rotation matrix given the angles (ùõº, ùõΩ, ùõæ) = (2œÄ/5, œÄ/18, œÄ/6) in radians (X-Y-Z). Do not use inbuilt functions.\n",
    "\n",
    "b. Solve for angles using fsolve from scipy for three initializations of your choice and compare.\n",
    "$$M(\\alpha , \\beta ,\\gamma)=\\left[\\begin{array}{rrr}0.26200263 & -0.19674724 &  0.944799  \\\\0.21984631 &  0.96542533  & 0.14007684 \\\\\n",
    "   -0.93969262 & 0.17101007 & 0.29619813\\end{array}\\right] \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# TODO: Do tasks described in 2.1 (a)\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# TODO: Do tasks described in 2.1 (b)\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Equivalent angle‚Äìaxis representation (2.5 points) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Write a function to convert equivalent angle‚Äìaxis representation (with a general axis and angle) to matrix form and vice versa. \\\n",
    "Try it for $\\theta = \\pi/6$ and axis $K= [1, 2, 3]^T $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# TODO: Do tasks described in 2.2 \n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Gimbal lock (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show an example where a Gimbal lock occurs and visualize the Gimbal lock on the given point cloud, data/toothless.ply. You have to show the above by animation (rotation along each axis one by one).\n",
    "\n",
    "**Hint:** \n",
    "Create 3 disks perpendicular to each other representing axes for local frame of object. Show that in certain configuration, due to use of Euler angles we can lose a degree of freedom. \n",
    "\n",
    "Use Open3D's non-blocking visualization and discretize the rotation to simulate the animation. For example, if you want to rotate by 20¬∞ around a particular axis, do so in increments of 5¬∞ 4 times to make it look like an animation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# TODO: Do tasks described in 2.3\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4: Quaternions (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Convert a rotation matrix to quaternion and vice versa. Do not use inbuilt libraries for this question.\n",
    "\n",
    "b. Perform matrix multiplication of two 3√ó3 rotation matrices and perform the same transformation in the quaternion space. Verify if the final transformation obtained in both cases is the same.\n",
    "\n",
    "c. Try to interpolate any given model between two rotation matrices and visualize!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# TODO: Do tasks described in 2.4 (a)\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# TODO: Do tasks described in 2.4 (b)\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# TODO: Do tasks described in 2.4 (c)\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3: Interpolation between transformations (15 points)\n",
    "\n",
    "Given 2 random transformation matrices, interpolate the given point cloud **toothless.ply** from `T2` to `T1` and visualize it.\n",
    "\n",
    "We will use the `generateTransformation()` function to generate a random Transformation matrix. You can write your own `generateTransformation()` function for testing, but we will replace it with our own so make sure that your code works for general cases.\n",
    "\n",
    "Ensure that your visualization shows the starting and ending configurations during interpolation.\n",
    "\n",
    "Your final output should look something like this:\n",
    "![Visualization](./out.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateTransformation():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "T1 = generateTransformation()\n",
    "T2 = generateTransformation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# TODO: Implement the above question using spherical linear interpolation (slerp)\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References: https://en.wikipedia.org/wiki/Slerp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "makeagent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
