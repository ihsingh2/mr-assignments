{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roll number: 2023122002, 2023121013, 2023121006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Instructions\n",
    " * Fill in the roll-number in the cell above.\n",
    " * Code must be submitted in Python in jupyter notebooks. We highly recommend using anaconda/miniconda distribution or at the minimum, virtual environments for this assignment.\n",
    " * All the code and result files should be uploaded in the github classroom.\n",
    " * For this assignment, you will be using Open3D  extensively. Refer to [Open3D](http://www.open3d.org/docs/release/) documentation.\n",
    " *  Most of the questions require you to **code your own functions** unless there is a need to call in the abilities of the mentioned libraries, such as Visualisation from Open3D. Make sure your code is modular since you will be reusing them for future assignments. All the functions related to transformation matrices, quaternions, and 3D projection are expected to be coded by you.\n",
    " *  All the representations are expected to be in a right-hand coordinate system.\n",
    "<!--  * Answer to the descriptive questions should be answered in your own words. Copy-paste answers will lead to penalty. -->\n",
    " * You could split the Jupyter Notebook cells where TODO is written, but please try to avoid splitting/changing the structure of other cells.\n",
    " * All the visualization should be done inside the notebook unless specified otherwise.\n",
    " * Plagiarism will lead to heavy penalty.\n",
    " * Commit the notebooks in the repo and any other results files under the result folder in the GitHub Classroom repo. \n",
    " * Commits past the deadline will not be considered.\n",
    " * This is a group assignment. Discussions are encouraged but any sharing of code among different teams will be penalized. \n",
    "\n",
    "### Instructions for group formation\n",
    " * We have circulated google sheet in moodle to fill in team members. Please finalize the teams formation by 18th Aug (tentative deadline). Same teams will be working towards project and other 2 Assignments as well. \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1: Transformations and Projections on Autonomous Driving Dataset (20 Points)\n",
    "\n",
    "In this question, you will work with real world autonomous driving dataset (sequence in Waymo dataset). The dataset has LiDAR point clouds, images. You are required to demonstrate: \n",
    "\n",
    "**I. Various transformations of rotation matrices as described in below tasks.**\n",
    "\n",
    "**II. Visualization as a result of above transformations in Open3D**\n",
    "\n",
    "## Given data:\n",
    "\n",
    "1.) `LiDAR Point Clouds` : Stored at each timestep in the folder `lidar`. The point clouds are provided in the ego frame attached to lidar sensor (vehicle's reference frame).\n",
    "\n",
    "2.) `Images` : Stored at each timestep in the folder `images`. \n",
    "\n",
    "**Naming Convention** : {timestep}_{cam_no}.jpg where timestep is specified in 3 digits and cam_no : [0, 1, 2] indicates centre, left and right camera respectively.\n",
    "\n",
    "3.) `Camera-to-Ego Transformations`: Stored in the folder `cam2ego`, which converts points from each camera's reference frame to the vehicle's (or ego) reference frame.\n",
    "\n",
    "4.) `Ego-to-World Transformations`: Stored in the folder `ego2world`, which converts points from the vehicle's reference frame to the world frame W.\n",
    "\n",
    "5.) `Camera Intrinsics`: Stored in the folder `intrinsics` provided for 3 cameras.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions to read lidar data and camera instrinsics are provided below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "from copy import copy\n",
    "import os\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from scipy.optimize import fsolve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def read_transformation(filepath):\n",
    "    data = np.loadtxt(filepath).reshape(4,4)\n",
    "    matrix = data.reshape(4, 4)\n",
    "    return matrix\n",
    "\n",
    "def read_cam2ego_transformation(camera):\n",
    "    return read_transformation(f'cam2ego/{camera}.txt')\n",
    "\n",
    "def read_ego2world_transformation(timestep):\n",
    "    return read_transformation(f'ego2world/{timestep:03d}.txt')\n",
    "\n",
    "def read_image(timestep, camera):\n",
    "    image = cv2.imread(f'images/{timestep:03d}_{camera}.jpg')\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    return image\n",
    "\n",
    "def display_image(image, title=''):\n",
    "    plt.imshow(image)\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def read_intrinsic(camera):\n",
    "    intrinsic = np.loadtxt(f'intrinsics/{camera}.txt')\n",
    "    fx, fy, cx, cy = intrinsic[0], intrinsic[1], intrinsic[2], intrinsic[3]\n",
    "    intrinsic_matrix = np.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]])\n",
    "    return intrinsic_matrix\n",
    "\n",
    "def read_lidar_data(timestep):\n",
    "    lidar_data = np.memmap(f'lidar/{timestep:03d}.bin',\n",
    "                    dtype=np.float32,\n",
    "                    mode=\"r\",\n",
    "                ).reshape(-1, 14)   # (165454, 14)\n",
    "\n",
    "    lidar_origins = lidar_data[:, :3]\n",
    "    lidar_points = lidar_data[:, 3:6]   # (165454, 3)\n",
    "    lidar_ids = lidar_data[:, -1]   # (165454,)\n",
    "    return lidar_origins, lidar_points, lidar_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Note:** Even though Waymo dataset has 5 cameras, you are given the dataset corresponding to middle 3 cameras only. Please ignore other 2 cameras.\n",
    "\n",
    "## Notation for tasks:\n",
    "\n",
    "a.) `Global Reference Frame G`: Defined as the first ego frame (i.e., the translation vector of ego2world[0] is the origin of frame G in world frame W).\n",
    "\n",
    "World Frame W: A fixed world reference frame.\n",
    "\n",
    "b.) `Ego Frame`: Attached to the LiDAR and changes as the vehicle moves.\n",
    "\n",
    "c.) `Camera Frames`: Each of the 5 cameras has its own frame, which changes as the vehicle moves.\n",
    "\n",
    "Note: Axis directions of `Ego Frame` and `Camera Frames` are aligned with the Waymo Coordinate System (LiDAR) described below\n",
    "\n",
    "## Coordinate Systems:\n",
    "\n",
    "**OpenCV Coordinate System:** x right, y down, z front.\n",
    "\n",
    "**Waymo Coordinate System (LiDAR):** x front, y left, z up.\n",
    "\n",
    "\n",
    "![Waymo Setup](./waymo_setup.jpg \"Waymo Setup\")\n",
    "\n",
    "Link to dataset (one sequence) : https://drive.google.com/drive/folders/17YDx2Yn1KmPjmlaHsoFz4Jpa8zpgovO2?usp=drive_link\n",
    "\n",
    "If you want to try on other sequences as well, please refer to : https://waymo.com/open/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Task 1`. Transformations of LiDAR Point Clouds (10 points)\n",
    "\n",
    "**Instructions:** \n",
    "\n",
    "Transform the LiDAR point clouds at all timesteps to the global reference frame G. Concatenate these transformed point clouds.\n",
    "    \n",
    "Visualization: Use Open3D to visualize the concatenated point cloud in the global reference frame G. Also, display the concatenation process at every timestep starting from first point cloud\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_transformation(points, transformation):\n",
    "    rotation = transformation[:3, :3]\n",
    "    translation = transformation[:3, 3]\n",
    "    transformed_points = (points @ rotation) + translation\n",
    "    return transformed_points\n",
    "\n",
    "def visualize_points(points, point_cloud):\n",
    "    if point_cloud is None:\n",
    "        point_cloud = o3d.geometry.PointCloud()\n",
    "        point_cloud.points = o3d.utility.Vector3dVector(points)\n",
    "        vis.add_geometry(point_cloud)\n",
    "    else:\n",
    "        point_cloud.points = o3d.utility.Vector3dVector(points)\n",
    "        vis.update_geometry(point_cloud)\n",
    "    vis.poll_events()\n",
    "    vis.update_renderer()\n",
    "    time.sleep(0.1)\n",
    "    return point_cloud\n",
    "\n",
    "vis = o3d.visualization.Visualizer()\n",
    "vis.create_window()\n",
    "point_cloud = None\n",
    "\n",
    "num_timesteps = 10\n",
    "all_points = np.empty((0, 3))\n",
    "\n",
    "for timestep in range(num_timesteps):\n",
    "    _, points, _ = read_lidar_data(timestep)\n",
    "    transformation = read_ego2world_transformation(timestep)\n",
    "    transformed_points = apply_transformation(points, transformation)\n",
    "    all_points = np.concatenate([all_points, transformed_points])\n",
    "    point_cloud = visualize_points(all_points, point_cloud)\n",
    "\n",
    "vis.run()\n",
    "vis.destroy_window()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Task 2`. Projecting LiDAR Point Clouds onto images (10 points)\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "Transform the concatenated point cloud from task 1 to the frame of each of the 3 cameras at timesteps `0, 20, and 55`. \n",
    "    \n",
    "Project these transformed point clouds onto the respective camera frames using the provided camera intrinsics. Concatenated point cloud would be very dense, so randomly select arbitrary number of points for better visualization. \n",
    "\n",
    "**Projected image pixel x : K * X_3d where X_3d is the 3d point in camera frame.**\n",
    "    \n",
    "Visualization: Overlay the projected points onto the camera images and visualize them.\n",
    "\n",
    "**For example:** Overlayed concatenated point cloud on camera `000_0.png` and `030_2.png` are shown below\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"./projected_000_0.png\" alt=\"Drawing\" style=\"width: 750px;\"/> </td>\n",
    "<td> <img src=\"./projected_030_2.png\" alt=\"Drawing\" style=\"width: 750px;\"/> </td>\n",
    "</tr></table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# TODO: TASK 2\n",
    "##############################################################################\n",
    "\n",
    "def apply_inverse_transformation(points, transformation):\n",
    "    rotation = transformation[:3, :3]\n",
    "    translation = transformation[:3, 3]\n",
    "    transformed_points = (points - translation) @ (rotation.T)\n",
    "    return transformed_points\n",
    "\n",
    "def image_projection(points, intrinsic, image_height, image_width):\n",
    "    image_points = points @ intrinsic.T\n",
    "    image_points[:, :2] /= np.where(image_points[:, [2]] != 0, image_points[:, [2]], 1)\n",
    "    image_points = np.rint(image_points).astype(int)\n",
    "    in_range_mask = (\n",
    "        (0 <= image_points[:, 0]) & (image_points[:, 0] < image_height) &\n",
    "        (0 <= image_points[:, 1]) & (image_points[:, 1] < image_width) &\n",
    "        (0 <= image_points[:, 2])\n",
    "    )\n",
    "    image_points_masked = image_points[in_range_mask]\n",
    "    return image_points_masked\n",
    "\n",
    "def visualize_projection(image, image_points, sample_fraction=0.05, marker_size=0.2):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sample_size = int(sample_fraction * image_points.shape[0])\n",
    "    sample_indices = np.random.choice(image_points.shape[0], size=sample_size, replace=False)\n",
    "    sampled_image_points = image_points[sample_indices]\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.scatter(sampled_image_points[:, 1], sampled_image_points[:, 0], c='lime', s=marker_size)\n",
    "    plt.show()\n",
    "\n",
    "timestep = 0\n",
    "camera = 0\n",
    "\n",
    "ego_transformation = read_ego2world_transformation(timestep)\n",
    "all_points_transformed = apply_inverse_transformation(all_points, ego_transformation)\n",
    "\n",
    "camera_transformation = read_cam2ego_transformation(camera)\n",
    "all_points_transformed = apply_inverse_transformation(all_points_transformed, camera_transformation)\n",
    "\n",
    "waymo_to_cv = np.array([\n",
    "    [0.0, -1.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, -1.0, 0.0],\n",
    "    [1.0, 0.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 1.0]\n",
    "], dtype=np.float32)\n",
    "all_points_transformed = apply_transformation(all_points_transformed, waymo_to_cv)\n",
    "\n",
    "intrinsic = read_intrinsic(camera)\n",
    "image = read_image(timestep, camera)\n",
    "image_points = image_projection(all_points_transformed, intrinsic, image.shape[0], image.shape[1])\n",
    "\n",
    "visualize_projection(image, image_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "\n",
    "### `Task 3`. Compute Depth Image from Projected Point Cloud in camera frame (5 points)\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "Using the projected point clouds to camera frame from task 2, visualize the depth image by considering only the z-coordinate of the projected points in the camera frame.\n",
    "\n",
    "Visualization: Display the depth image for each of the 3 cameras at timesteps `0, 20, and 55` alongside the corresponding RGB image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# TODO: TASK 3\n",
    "##############################################################################\n",
    "\n",
    "def apply_inverse_transformation(points, transformation):\n",
    "    rotation = transformation[:3, :3]\n",
    "    translation = transformation[:3, 3]\n",
    "    transformed_points = (points - translation) @ (rotation.T)\n",
    "    return transformed_points\n",
    "\n",
    "def image_projection(points, intrinsic, image_height, image_width):\n",
    "    image_points = points @ intrinsic.T\n",
    "    image_points[:, :2] /= np.where(image_points[:, [2]] != 0, image_points[:, [2]], 1)\n",
    "    image_points = np.rint(image_points).astype(int)\n",
    "    in_range_mask = (\n",
    "        (0 <= image_points[:, 0]) & (image_points[:, 0] < image_height) &\n",
    "        (0 <= image_points[:, 1]) & (image_points[:, 1] < image_width) &\n",
    "        (0 <= image_points[:, 2])\n",
    "    )\n",
    "    image_points_masked = image_points[in_range_mask]\n",
    "    return image_points_masked\n",
    "\n",
    "def visualize_projection(image, image_points, sample_fraction=0.05, marker_size=0.2):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sample_size = int(sample_fraction * image_points.shape[0])\n",
    "    sample_indices = np.random.choice(image_points.shape[0], size=sample_size, replace=False)\n",
    "    sampled_image_points = image_points[sample_indices]\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.scatter(sampled_image_points[:, 1], sampled_image_points[:, 0], c=sampled_image_points[:, 2], s=marker_size, cmap='inferno')\n",
    "    plt.show()\n",
    "\n",
    "timestep = 0\n",
    "camera = 0\n",
    "\n",
    "ego_transformation = read_ego2world_transformation(timestep)\n",
    "all_points_transformed = apply_inverse_transformation(all_points, ego_transformation)\n",
    "\n",
    "camera_transformation = read_cam2ego_transformation(camera)\n",
    "all_points_transformed = apply_inverse_transformation(all_points_transformed, camera_transformation)\n",
    "\n",
    "waymo_to_cv = np.array([\n",
    "    [0.0, -1.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, -1.0, 0.0],\n",
    "    [1.0, 0.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 1.0]\n",
    "], dtype=np.float32)\n",
    "all_points_transformed = apply_transformation(all_points_transformed, waymo_to_cv)\n",
    "\n",
    "intrinsic = read_intrinsic(camera)\n",
    "image = read_image(timestep, camera)\n",
    "image_points = image_projection(all_points_transformed, intrinsic, image.shape[0], image.shape[1])\n",
    "\n",
    "visualize_projection(image, image_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: You might be asked to show the above results for different timesteps and from one of the 3 cameras during evaluation/viva."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2: Various Representations for Rotations and Gimbal lock (15 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Euler angles (2.5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Write a function that returns a rotation matrix given the angles (ð›¼, ð›½, ð›¾) = (2Ï€/5, Ï€/18, Ï€/6) in radians (X-Y-Z). Do not use inbuilt functions.\n",
    "\n",
    "b. Solve for angles using fsolve from scipy for three initializations of your choice and compare.\n",
    "$$M(\\alpha , \\beta ,\\gamma)=\\left[\\begin{array}{rrr}0.26200263 & -0.19674724 &  0.944799  \\\\0.21984631 &  0.96542533  & 0.14007684 \\\\\n",
    "   -0.93969262 & 0.17101007 & 0.29619813\\end{array}\\right] \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rotation Matrix:\n",
      "[[ 0.85286853 -0.49240388  0.17364818]\n",
      " [ 0.29753193  0.18504195 -0.93660783]\n",
      " [ 0.42905713  0.85046922  0.30432233]]\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# TODO: Do tasks described in 2.1 (a)\n",
    "##############################################################################\n",
    "\n",
    "def rotation_X(alpha):\n",
    "    return np.array([\n",
    "        [1, 0, 0],\n",
    "        [0, np.cos(alpha), -np.sin(alpha)],\n",
    "        [0, np.sin(alpha),  np.cos(alpha)]\n",
    "    ])\n",
    "\n",
    "def rotation_Y(beta):\n",
    "    return np.array([\n",
    "        [ np.cos(beta), 0, np.sin(beta)],\n",
    "        [0, 1, 0],\n",
    "        [-np.sin(beta), 0, np.cos(beta)]\n",
    "    ])\n",
    "\n",
    "def rotation_Z(gamma):\n",
    "    return np.array([\n",
    "        [ np.cos(gamma), -np.sin(gamma), 0],\n",
    "        [ np.sin(gamma),  np.cos(gamma), 0],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "\n",
    "def rotation_matrix(angles):\n",
    "    alpha, beta, gamma = angles\n",
    "    R = rotation_X(alpha) @ rotation_Y(beta) @ rotation_Z(gamma)\n",
    "    return R\n",
    "\n",
    "alpha = 2 * np.pi / 5\n",
    "beta = np.pi / 18\n",
    "gamma = np.pi / 6\n",
    "\n",
    "R = rotation_matrix([alpha, beta, gamma])\n",
    "print('Rotation Matrix:')\n",
    "print(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# TODO: Do tasks described in 2.1 (b)\n",
    "##############################################################################\n",
    "\n",
    "def rotation_matrix_error(angles, target_matrix):\n",
    "    R = rotation_matrix(angles)\n",
    "    return np.abs(np.sum(R - target_matrix, axis=0))\n",
    "\n",
    "M = np.array([\n",
    "    [ 0.26200263, -0.19674724,  0.944799],\n",
    "    [ 0.21984631,  0.96542533,  0.14007684],\n",
    "    [-0.93969262,  0.17101007,  0.29619813]\n",
    "])\n",
    "\n",
    "initializations = [\n",
    "    [0,                 0,              0],\n",
    "    [np.pi / 6,     np.pi,  3 * np.pi / 2],\n",
    "    [    np.pi, np.pi / 3,      np.pi / 4]\n",
    "]\n",
    "\n",
    "for angles in initializations:\n",
    "    solution = fsolve(rotation_matrix_error, angles, args=(M,))\n",
    "    error = rotation_matrix_error(solution, M).sum()\n",
    "    solution %= 2 * np.pi\n",
    "    # solution = np.degrees(solution)\n",
    "    print(f'Initialization: {angles}')\n",
    "    print(f'Solution: {solution}')\n",
    "    print(f'Error: {error}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Equivalent angleâ€“axis representation (2.5 points) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Write a function to convert equivalent angleâ€“axis representation (with a general axis and angle) to matrix form and vice versa. \\\n",
    "Try it for $\\theta = \\pi/6$ and axis $K= [1, 2, 3]^T $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Angle-Axis representation:\n",
      "Angle: 0.5235987755982988\n",
      "Axis: [0.26726124 0.53452248 0.80178373]\n",
      "\n",
      "Rotation matrix:\n",
      "[[ 0.87559502 -0.38175263  0.29597008]\n",
      " [ 0.42003109  0.90430386 -0.07621294]\n",
      " [-0.2385524   0.19104831  0.95215193]]\n",
      "\n",
      "Conversion to Angle-Axis:\n",
      "Angle: 0.5235987755982985 radians\n",
      "Normalized Axis: [0.26726124 0.53452248 0.80178373]\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# TODO: Do tasks described in 2.2 \n",
    "##############################################################################\n",
    "\n",
    "def angle_axis_to_matrix(angle, axis):\n",
    "    # Normalize the axis\n",
    "    axis = np.array(axis, dtype=np.float64)\n",
    "    axis /= np.linalg.norm(axis)\n",
    "\n",
    "    # Compute the skew-symmetric matrix\n",
    "    K = np.array([\n",
    "        [       0, -axis[2],  axis[1]],\n",
    "        [ axis[2],        0, -axis[0]],\n",
    "        [-axis[1],  axis[0],        0]\n",
    "    ])\n",
    "\n",
    "    # Rodrigues' rotation formula\n",
    "    I = np.eye(3)\n",
    "    sin_term = np.sin(angle) * K\n",
    "    cos_term = (1 - np.cos(angle)) * (K @ K)\n",
    "    R = I + sin_term + cos_term\n",
    "\n",
    "    return R\n",
    "\n",
    "def matrix_to_angle_axis(R):\n",
    "    # Angle\n",
    "    angle = np.arccos((np.trace(R) - 1) / 2)\n",
    "\n",
    "    # When near 0 - no rotation\n",
    "    if np.isclose(angle, 0):\n",
    "        return 0, np.array([1, 0, 0])\n",
    "\n",
    "    # When near 180 degrees\n",
    "    elif np.isclose(angle, np.pi):\n",
    "        eigenvalues, eigenvectors = np.linalg.eig(R)\n",
    "        axis = eigenvectors[:, np.isclose(eigenvalues, 1)].real.flatten()\n",
    "        return np.pi, axis\n",
    "\n",
    "    # Finding the axis\n",
    "    axis = (1 / (2 * np.sin(angle))) * np.array([\n",
    "        R[2, 1] - R[1, 2],\n",
    "        R[0, 2] - R[2, 0],\n",
    "        R[1, 0] - R[0, 1]\n",
    "    ])\n",
    "\n",
    "    return angle, axis\n",
    "\n",
    "# Initial values\n",
    "angle = np.pi / 6\n",
    "axis = np.array([1, 2, 3], dtype=np.float64)\n",
    "axis /= np.linalg.norm(axis)\n",
    "print(\"Original Angle-Axis representation:\")\n",
    "print(f\"Angle: {angle}\")\n",
    "print(f\"Axis: {axis}\")\n",
    "\n",
    "# Find rotation matrix\n",
    "R = angle_axis_to_matrix(angle, axis)\n",
    "print(\"\\nRotation matrix:\")\n",
    "print(R)\n",
    "\n",
    "# Find Angle-Axis representation\n",
    "angle, axis = matrix_to_angle_axis(R)\n",
    "print(\"\\nConversion to Angle-Axis:\")\n",
    "print(f\"Angle: {angle} radians\")\n",
    "print(f\"Normalized Axis: {axis}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Gimbal lock (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show an example where a Gimbal lock occurs and visualize the Gimbal lock on the given point cloud, data/toothless.ply. You have to show the above by animation (rotation along each axis one by one).\n",
    "\n",
    "**Hint:** \n",
    "Create 3 disks perpendicular to each other representing axes for local frame of object. Show that in certain configuration, due to use of Euler angles we can lose a degree of freedom. \n",
    "\n",
    "Use Open3D's non-blocking visualization and discretize the rotation to simulate the animation. For example, if you want to rotate by 20Â° around a particular axis, do so in increments of 5Â° 4 times to make it look like an animation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# TODO: Do tasks described in 2.3\n",
    "##############################################################################\n",
    "\n",
    "# Initialize torus\n",
    "torus_x = o3d.geometry.TriangleMesh.create_torus(torus_radius=350.0, tube_radius=5.0)\n",
    "torus_y = o3d.geometry.TriangleMesh.create_torus(torus_radius=325.0, tube_radius=5.0)\n",
    "torus_z = o3d.geometry.TriangleMesh.create_torus(torus_radius=300.0, tube_radius=5.0)\n",
    "\n",
    "# Assign colors to torus\n",
    "torus_x.paint_uniform_color([1.0, 0.0, 0.0])\n",
    "torus_y.paint_uniform_color([0.0, 1.0, 0.0])\n",
    "torus_z.paint_uniform_color([0.0, 0.0, 1.0])\n",
    "\n",
    "# Initialize axes, associated with torus\n",
    "x_axis = np.array([1, 0, 0])\n",
    "y_axis = np.array([0, 1, 0])\n",
    "z_axis = np.array([0, 0, 1])\n",
    "\n",
    "# Align torus with respective axes\n",
    "torus_x.vertices = o3d.utility.Vector3dVector(np.asarray(torus_x.vertices) @ angle_axis_to_matrix(np.pi / 2, y_axis).T)\n",
    "torus_y.vertices = o3d.utility.Vector3dVector(np.asarray(torus_y.vertices) @ angle_axis_to_matrix(np.pi / 2, x_axis).T)\n",
    "\n",
    "# Initialize point cloud\n",
    "point_cloud = o3d.io.read_point_cloud('toothless.ply')\n",
    "\n",
    "# Initialize visualization\n",
    "vis = o3d.visualization.Visualizer()\n",
    "vis.create_window()\n",
    "vis.add_geometry(point_cloud)\n",
    "vis.add_geometry(torus_x)\n",
    "vis.add_geometry(torus_y)\n",
    "vis.add_geometry(torus_z)\n",
    "\n",
    "# Sequence of rotations\n",
    "axes = [('y', 90), ('x', 45), ('z', 45)]\n",
    "\n",
    "# Iterate over all rotation\n",
    "for ax, degree in axes:\n",
    "\n",
    "    # Rotate slowly until entire rotation complete\n",
    "    angle = 0\n",
    "    step = 5\n",
    "    while angle < degree:\n",
    "        angle += step\n",
    "\n",
    "        # Compute rotation matrix\n",
    "        if ax == \"x\":\n",
    "            rotation = angle_axis_to_matrix(np.radians(step), x_axis)\n",
    "        elif ax == \"y\":\n",
    "            rotation = angle_axis_to_matrix(np.radians(step), y_axis)\n",
    "        elif ax == \"z\":\n",
    "            rotation = angle_axis_to_matrix(np.radians(step), z_axis)\n",
    "\n",
    "        # Rotate the gimbals and point cloud\n",
    "        if ax == 'x':\n",
    "            torus_x.vertices = o3d.utility.Vector3dVector(np.asarray(torus_x.vertices) @ rotation.T)\n",
    "            x_axis = x_axis @ rotation\n",
    "        if ax == 'x' or ax == 'y':\n",
    "            torus_y.vertices = o3d.utility.Vector3dVector(np.asarray(torus_y.vertices) @ rotation.T)\n",
    "            y_axis = y_axis @ rotation\n",
    "        torus_z.vertices = o3d.utility.Vector3dVector(np.asarray(torus_z.vertices) @ rotation.T)\n",
    "        point_cloud.points = o3d.utility.Vector3dVector(np.asarray(point_cloud.points) @ rotation.T)\n",
    "        z_axis = z_axis @ rotation\n",
    "\n",
    "        # Update the visualizer\n",
    "        vis.update_geometry(point_cloud)\n",
    "        vis.update_geometry(torus_x)\n",
    "        vis.update_geometry(torus_y)\n",
    "        vis.update_geometry(torus_z)\n",
    "\n",
    "        # Update renderer\n",
    "        vis.poll_events()\n",
    "        vis.update_renderer()\n",
    "        time.sleep(0.1)\n",
    "\n",
    "# Terminate\n",
    "vis.run()\n",
    "vis.destroy_window()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4: Quaternions (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Convert a rotation matrix to quaternion and vice versa. Do not use inbuilt libraries for this question.\n",
    "\n",
    "b. Perform matrix multiplication of two 3Ã—3 rotation matrices and perform the same transformation in the quaternion space. Verify if the final transformation obtained in both cases is the same.\n",
    "\n",
    "c. Try to interpolate any given model between two rotation matrices and visualize!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# TODO: Do tasks described in 2.4 (a)\n",
    "##############################################################################\n",
    "\n",
    "def rotation_matrix_to_quaternions(R):\n",
    "    trace = np.trace(R)\n",
    "\n",
    "    if trace > 0:\n",
    "        S = np.sqrt(trace + 1.0) * 2\n",
    "        w = 0.25 * S\n",
    "        x = (R[2, 1] - R[1, 2]) / S\n",
    "        y = (R[0, 2] - R[2, 0]) / S\n",
    "        z = (R[1, 0] - R[0, 1]) / S\n",
    "    elif R[0, 0] > R[1, 1] and R[0, 0] > R[2, 2]:\n",
    "        S = np.sqrt(1.0 + R[0, 0] - R[1, 1] - R[2, 2]) * 2\n",
    "        w = (R[2, 1] - R[1, 2]) / S\n",
    "        x = 0.25 * S\n",
    "        y = (R[0, 1] + R[1, 0]) / S\n",
    "        z = (R[0, 2] + R[2, 0]) / S\n",
    "    elif R[1, 1] > R[2, 2]:\n",
    "        S = np.sqrt(1.0 + R[1, 1] - R[0, 0] - R[2, 2]) * 2\n",
    "        w = (R[0, 2] - R[2, 0]) / S\n",
    "        x = (R[0, 1] + R[1, 0]) / S\n",
    "        y = 0.25 * S\n",
    "        z = (R[1, 2] + R[2, 1]) / S\n",
    "    else:\n",
    "        S = np.sqrt(1.0 + R[2, 2] - R[0, 0] - R[1, 1]) * 2\n",
    "        w = (R[1, 0] - R[0, 1]) / S\n",
    "        x = (R[0, 2] + R[2, 0]) / S\n",
    "        y = (R[1, 2] + R[2, 1]) / S\n",
    "        z = 0.25 * S\n",
    "\n",
    "    return np.array([w, x, y, z])\n",
    "\n",
    "def quaternions_to_rotation_matrix(q):\n",
    "    qw, qx, qy, qz = q\n",
    "    R = np.array([[1 - 2*qy**2 - 2*qz**2,   2*qx*qy - 2*qz*qw,          2*qx*qz + 2*qy*qw],\n",
    "                  [2*qx*qy + 2*qz*qw,       1 - 2*qx**2 - 2*qz**2,      2*qy*qz - 2*qx*qw],\n",
    "                  [2*qx*qz - 2*qy*qw,       2*qy*qz + 2*qx*qw,          1 - 2*qx**2 - 2*qy**2]])\n",
    "    return R\n",
    "\n",
    "# Initialization\n",
    "R = np.array([[0.866, -0.5, 0],\n",
    "              [0.5, 0.866, 0],\n",
    "              [0, 0, 1]])\n",
    "\n",
    "# Convert to Quaternion\n",
    "q = rotation_matrix_to_quaternions(R)\n",
    "print(\"Quaternion:\", q)\n",
    "\n",
    "# Convert to Rotation Matrix\n",
    "R_converted = quaternions_to_rotation_matrix(q)\n",
    "print(\"Rotation Matrix:\\n\", R_converted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# TODO: Do tasks described in 2.4 (b)\n",
    "##############################################################################\n",
    "\n",
    "def multiply_rotation_matrix(R1, R2):\n",
    "    return R1 @ R2\n",
    "\n",
    "def multiply_quaternions(q1, q2):\n",
    "    w1, x1, y1, z1 = q1\n",
    "    w2, x2, y2, z2 = q2\n",
    "    w = w1*w2 - x1*x2 - y1*y2 - z1*z2\n",
    "    x = w1*x2 + x1*w2 + y1*z2 - z1*y2\n",
    "    y = w1*y2 - x1*z2 + y1*w2 + z1*x2\n",
    "    z = w1*z2 + x1*y2 - y1*x2 + z1*w2\n",
    "    return np.array([w, x, y, z])\n",
    "\n",
    "# Initialize rotation matrix\n",
    "R1 = np.array([[0.866, -0.5, 0],\n",
    "               [0.5, 0.866, 0],\n",
    "               [0, 0, 1]])\n",
    "R2 = np.array([[1, 0, 0],\n",
    "               [0, 0.866, -0.5],\n",
    "               [0, 0.5, 0.866]])\n",
    "RProduct = multiply_rotation_matrix(R1, R2)\n",
    "print(\"Multiplication of Rotation Matrices:\\n\", RProduct)\n",
    "\n",
    "# Convert to quaternion and perform multiplication\n",
    "q1 = rotation_matrix_to_quaternions(R1)\n",
    "q2 = rotation_matrix_to_quaternions(R2)\n",
    "q_product = multiply_quaternions(q1, q2)\n",
    "print(\"\\nMultiplication of Quaternions:\", q_product)\n",
    "\n",
    "# Convert quaternion product back to rotation matrix\n",
    "R_from_quaternion_product = quaternions_to_rotation_matrix(q_product)\n",
    "print(\"\\nRotation Matrix from multiplication of Quaternions: \\n\", R_from_quaternion_product)\n",
    "print(\"\\nDifference betwen Rotation Matrix and Quaternion Product: \\n\", RProduct -  R_from_quaternion_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# TODO: Do tasks described in 2.4 (c)\n",
    "##############################################################################\n",
    "\n",
    "# Function to interpolate between two rotation matrices\n",
    "def interpolate_rotation_matrix(R1, R2, t):\n",
    "    return (1 - t) * R1 + t * R2\n",
    "\n",
    "# First Rotation matrix\n",
    "R1 = np.array([[0.866, -0.5, 0],\n",
    "               [0.5, 0.866, 0],\n",
    "               [0, 0, 1]])\n",
    "\n",
    "# Second Rotation matrix\n",
    "R2 = np.array([[1, 0, 0],\n",
    "               [0, 0.866, -0.5],\n",
    "               [0, 0.5, 0.866]])\n",
    "\n",
    "# Initialize the visualization\n",
    "pcd = o3d.io.read_point_cloud(\"toothless.ply\")\n",
    "vis = o3d.visualization.Visualizer()\n",
    "vis.create_window()\n",
    "vis.add_geometry(pcd)\n",
    "\n",
    "# Interpolate\n",
    "t_values = np.linspace(0, 1, 50)\n",
    "for t in t_values:\n",
    "    R_interp = interpolate_rotation_matrix(R1, R2, t)\n",
    "    pcd.rotate(R_interp, center=(0, 0, 0))\n",
    "    vis.update_geometry(pcd)\n",
    "    vis.poll_events()\n",
    "    vis.update_renderer()\n",
    "    time.sleep(0.1)\n",
    "\n",
    "# Terminate\n",
    "vis.run()\n",
    "vis.destroy_window()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3: Interpolation between transformations (15 points)\n",
    "\n",
    "Given 2 random transformation matrices, interpolate the given point cloud **toothless.ply** from `T2` to `T1` and visualize it.\n",
    "\n",
    "We will use the `generateTransformation()` function to generate a random Transformation matrix. You can write your own `generateTransformation()` function for testing, but we will replace it with our own so make sure that your code works for general cases.\n",
    "\n",
    "Ensure that your visualization shows the starting and ending configurations during interpolation.\n",
    "\n",
    "Your final output should look something like this:\n",
    "![Visualization](./out.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateTransformation():\n",
    "    \"\"\" Generate a random 4x4 transformation matrix. \"\"\"\n",
    "\n",
    "    R = o3d.geometry.get_rotation_matrix_from_xyz(np.random.rand(3) * 2 * np.pi)\n",
    "    T = np.random.rand(3) * 5000\n",
    "    transformation = np.vstack((np.hstack((R, T.reshape(-1, 1))), [0, 0, 0, 1]))\n",
    "    return transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_transformations(T1, T2, num_steps):\n",
    "    \"\"\" Interpolate between two transformations. \"\"\"\n",
    "\n",
    "    interpolated_transforms = []\n",
    "    for t in np.linspace(0, 1, num_steps):\n",
    "        rot_1, trans_1 = T1[:3, :3], T1[:3, 3]\n",
    "        rot_2, trans_2 = T2[:3, :3], T2[:3, 3]\n",
    "        R = rot_1 * (1 - t) + rot_2 * t\n",
    "        T = trans_1 * (1 - t) + trans_2 * t\n",
    "        transform = np.vstack((np.hstack((R, T.reshape(-1, 1))), [0, 0, 0, 1]))\n",
    "        interpolated_transforms.append(transform)\n",
    "    return interpolated_transforms\n",
    "\n",
    "def apply_transformation(pcd, transformation):\n",
    "    \"\"\" Apply a transformation matrix to copy of a point cloud. \"\"\"\n",
    "\n",
    "    new_pcd = o3d.geometry.PointCloud()\n",
    "    new_pcd.points = pcd.points\n",
    "    new_pcd.colors = pcd.colors\n",
    "    new_pcd = new_pcd.transform(transformation)\n",
    "    return new_pcd\n",
    "\n",
    "# Generate two random transformations\n",
    "T1 = generateTransformation()\n",
    "T2 = generateTransformation()\n",
    "\n",
    "# Load the point cloud\n",
    "point_cloud = o3d.io.read_point_cloud(\"toothless.ply\")\n",
    "original_points = copy(point_cloud.points)\n",
    "print(type(original_points))\n",
    "\n",
    "# Apply initial and final transformations\n",
    "initial_point_cloud = apply_transformation(point_cloud, T1)\n",
    "final_point_cloud = apply_transformation(point_cloud, T2)\n",
    "\n",
    "# Create visualizer\n",
    "vis = o3d.visualization.Visualizer()\n",
    "vis.create_window()\n",
    "vis.add_geometry(point_cloud)\n",
    "vis.add_geometry(initial_point_cloud)\n",
    "vis.add_geometry(final_point_cloud)\n",
    "\n",
    "# Interpolate between T1 and T2\n",
    "num_steps = 50\n",
    "interpolated_transforms = interpolate_transformations(T1, T2, num_steps)\n",
    "\n",
    "# Visualize interpolation\n",
    "for transform in interpolated_transforms:\n",
    "    point_cloud.points = original_points\n",
    "    point_cloud.transform(transform)\n",
    "    vis.update_geometry(point_cloud)\n",
    "    vis.poll_events()\n",
    "    vis.update_renderer()\n",
    "    time.sleep(0.1)\n",
    "\n",
    "# Terminate\n",
    "vis.run()\n",
    "vis.destroy_window()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# TODO: Implement the above question using spherical linear interpolation (slerp)\n",
    "##############################################################################\n",
    "\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "def slerp(q1, q2, t):\n",
    "    \"\"\" Perform spherical linear interpolation between two quaternions. \"\"\"\n",
    "\n",
    "    # Compute the dot product to determine the angle between the quaternions\n",
    "    dot = np.dot(q1, q2)\n",
    "    \n",
    "    # Adjust for the case when the dot product is negative\n",
    "    if dot < 0.0:\n",
    "        q2 = -q2\n",
    "        dot = -dot\n",
    "\n",
    "    # Clamp dot to the range [0, 1] to avoid numerical issues\n",
    "    dot = np.clip(dot, 0.0, 1.0)\n",
    "\n",
    "    # Compute the interpolation coefficients\n",
    "    theta_0 = np.arccos(dot)\n",
    "    theta = theta_0 * t\n",
    "    q2 = q2 - q1 * dot\n",
    "    q2 = q2 / np.linalg.norm(q2)\n",
    "\n",
    "    # Compute the interpolated quaternion\n",
    "    q_interp = q1 * np.cos(theta) + q2 * np.sin(theta)\n",
    "    q_interp = q_interp / np.linalg.norm(q_interp)\n",
    "\n",
    "    return q_interp\n",
    "\n",
    "def interpolate_transformations(T1, T2, num_steps):\n",
    "    \"\"\" Interpolate between two transformations. \"\"\"\n",
    "    interpolated_transforms = []\n",
    "    R1, T1_part = T1[:3, :3], T1[:3, 3]\n",
    "    R2, T2_part = T2[:3, :3], T2[:3, 3]\n",
    "\n",
    "    # Convert rotation matrices to quaternions\n",
    "    q1 = R.from_matrix(R1).as_quat()\n",
    "    q2 = R.from_matrix(R2).as_quat()\n",
    "\n",
    "    for t in np.linspace(0, 1, num_steps):\n",
    "        # Interpolate rotation quaternions using SLERP\n",
    "        q_interp = slerp(q1, q2, t)\n",
    "        R_interp = R.from_quat(q_interp).as_matrix()\n",
    "\n",
    "        # Interpolate translations\n",
    "        T_interp = T1_part * (1 - t) + T2_part * t\n",
    "\n",
    "        # Combine rotation and translation into a 4x4 transformation matrix\n",
    "        transform = np.vstack((np.hstack((R_interp, T_interp.reshape(-1, 1))), [0, 0, 0, 1]))\n",
    "        interpolated_transforms.append(transform)\n",
    "    \n",
    "    return interpolated_transforms\n",
    "\n",
    "def apply_transformation(pcd, transformation):\n",
    "    \"\"\" Apply a transformation matrix to a point cloud. \"\"\"\n",
    "\n",
    "    new_pcd = o3d.geometry.PointCloud()\n",
    "    new_pcd.points = pcd.points\n",
    "    new_pcd.colors = pcd.colors\n",
    "    new_pcd = new_pcd.transform(transformation)\n",
    "    return new_pcd\n",
    "\n",
    "# Generate two random transformations\n",
    "T1 = generateTransformation()\n",
    "T2 = generateTransformation()\n",
    "\n",
    "# Load the point cloud\n",
    "point_cloud = o3d.io.read_point_cloud(\"toothless.ply\")\n",
    "original_points = copy(point_cloud.points)\n",
    "print(type(original_points))\n",
    "\n",
    "# Apply initial and final transformations\n",
    "initial_point_cloud = apply_transformation(point_cloud, T1)\n",
    "final_point_cloud = apply_transformation(point_cloud, T2)\n",
    "\n",
    "# Create visualizer\n",
    "vis = o3d.visualization.Visualizer()\n",
    "vis.create_window()\n",
    "vis.add_geometry(point_cloud)\n",
    "vis.add_geometry(initial_point_cloud)\n",
    "vis.add_geometry(final_point_cloud)\n",
    "\n",
    "# Interpolate between T1 and T2\n",
    "num_steps = 50\n",
    "interpolated_transforms = interpolate_transformations(T1, T2, num_steps)\n",
    "\n",
    "# Visualize interpolation\n",
    "for transform in interpolated_transforms:\n",
    "    point_cloud.points = original_points\n",
    "    point_cloud.transform(transform)\n",
    "    vis.update_geometry(point_cloud)\n",
    "    vis.poll_events()\n",
    "    vis.update_renderer()\n",
    "    time.sleep(0.1)\n",
    "\n",
    "# Terminate\n",
    "vis.run()\n",
    "vis.destroy_window()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References: https://en.wikipedia.org/wiki/Slerp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
